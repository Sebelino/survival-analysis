---
title: "Survival Analysis: Take-home Exam"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
editor_options:
  markdown:
    wrap: 72
bibliography: references.bib
csl: "vancouver-brackets.csl"
---

# AI disclosure

ChatGPT was used to review my solutions and for grammar and clarity
checks.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
```

# A.1.

Let $\lambda_1 = \lambda\exp(\beta x)$. Then the proportional hazards
model has a survival time function of the form:

```{=tex}
\begin{align*}
    & S(t|x) \\
    =& S_0(t)^{\exp(\beta x)} \\
    =& \exp(-\lambda t^k)^{\exp(\beta x)} \\
    =& \exp(-\lambda t^k\exp(\beta x)) \\
    =& \exp(-\lambda\exp(\beta x) t^k) \\
    =& \exp(-\lambda_1 t^k) \\
\end{align*}
```
which has the form of a Weibull distribution with shape parameter $k$
and scale parameter $\lambda_1 = \lambda\exp(-k\tilde{\beta} x)$.

# A.2.

Let $\lambda_2 = \lambda\exp(-k\tilde{\beta} x)$. Then the accelerated
survival time model has a survival time function of the form:

```{=tex}
\begin{align*}
    & S(t|x) \\
    =& S_0(t \exp(-\tilde{\beta} x)) \\
    =& \exp(-\lambda(t \exp(-\tilde{\beta} x))^k) \\
    =& \exp(-\lambda t^k \exp(-\tilde{\beta} x)^k) \\
    =& \exp(-\lambda t^k \exp(-k\tilde{\beta} x)) \\
    =& \exp(-\lambda\exp(-k\tilde{\beta} x) t^k) \\
    =& \exp(-\lambda_2 t^k ) \\
\end{align*}
```
which has the form of a Weibull distribution with shape parameter $k$
and scale parameter $\lambda_2 = \lambda\exp(-k\tilde{\beta} x)$.

# A.3.

Proportional hazards model survival function:
$$\exp(-\lambda\exp(\beta x) t^k)$$ Accelerated survival time model
survival function: $$\exp(-\lambda\exp(-k\tilde{\beta} x) t^k)$$ We see
that the survival functions are equal if the log hazard ratio is related
to the log time ratio like so: $$\beta = -k\tilde{\beta}$$ Given that
the shape parameter $k$ is positive, this means a positive $\beta$
implies a negative $\tilde{\beta}$.

# B.1.

Let $T$ be the survival time of an individual, so that $S(t) = P(T>t)$.

## a)

For individual $i$, we know that the event was observed in the interval
$(u_i,v_i]$, which means $u_i < T_i \leq v_i$. Furthermore, the event is
only observed if $T_i > t_i$, so we need to condition on $T_i>t_i$. The
likelihood contribution for a single data tuple $(t_i,u_i,v_i)$ is:
\begin{align*}
    & P(u_i < T_i \leq v_i \, | \, T_i > t_i) \\
    =& \frac{P(\{u_i < T_i \leq v_i\} \cap \{T_i > t_i\})}{P(T_i > t_i)} \\
    =& \frac{P(\max(t_i,u_i) < T_i \leq v_i)}{P(T_i > t_i)} \\
    =& \frac{P(u_i < T_i \leq v_i)}{P(T_i > t_i)} \\
    =& \frac{F_{T_i}(v_i)-F_{T_i}(u_i)}{P(T_i > t_i)} \\
    =& \frac{1-S(v_i)-(1-S(u_i))}{S(t_i)} \\
    =& \frac{S(u_i)-S(v_i)}{S(t_i)} \\
\end{align*}

Log-likelihood contribution for individual $i$: \begin{align*}
    & \ell_i \\
    =& \ln\frac{S(u_i)-S(v_i)}{S(t_i)} \\
    =& \ln(S(u_i) - S(v_i)) - \ln S(t_i) \\
\end{align*}

Log-likelihood contribution for $n$ individuals: \begin{align*}
    & \ell_{1:n} \\
    =& \ln\prod_{i=1}^n \frac{S(u_i)-S(v_i)}{S(t_i)} \\
    =& \sum_{i=1}^n \left(\ln(S(u_i) - S(v_i)) - \ln S(t_i)\right) \\
\end{align*}

## b)

Log-likelihood contribution for individual $i$: \begin{align*}
    & \ell_i \\
    =& \ln(S(u_i) - S(v_i)) - \ln S(t_i) \\
    =& \ln\left(S(u_i) - S(u_i)\exp\left(-\int_{u_i}^{v_i} h(t)dt\right)\right) - \ln S(t_i) \\
    =& \ln\left(S(u_i)\left(1 - \exp\left(-\int_{u_i}^{v_i} h(t)dt\right)\right)\right) - \ln S(t_i) \\
    =& \ln S(u_i) + \ln\left(1 - \exp\left(-\int_{u_i}^{v_i} h(t)dt\right)\right) - \ln S(t_i) \\
    =& \ln \exp\left(-\int_0^{u_i} h(t)dt\right) + \ln\left(1 - \exp\left(-\int_{u_i}^{v_i} h(t)dt\right)\right) - \ln \exp\left(-\int_0^{t_i} h(t)dt\right) \\
    =& -\int_0^{u_i} h(t)dt + \ln\left(1 - \exp\left(-\int_{u_i}^{v_i} h(t)dt\right)\right) +\int_0^{t_i} h(t)dt \\
    =& \ln\left(1 - \exp\left(-\int_{u_i}^{v_i} h(t)dt\right)\right) - \int_{t_i}^{u_i} h(t)dt
\end{align*}

<!-- TODO n individuals case as well? -->

# B.2.

```{r, message=FALSE, echo=FALSE}
library(survival)
```

According to the documentation [@survfun] and source code [@survivala]
for version `3.8-3` of the package, the function takes the following
arguments:

-   `time`: Number.
-   `time2`: Number.
-   `event`: Either 0, 1, 2, 3, TRUE or FALSE.
-   `type`: Any of the strings: `'right'`, `'left'`, `'interval'`,
    `'counting'`, `'interval2'`, `'mstate'`.
-   `origin`: Number.

Let us assume that it is possible to express the tuple $(t_i,u_i,v_i)$
using the `Surv` function. Then it is possible to do so by a function
call of the form:

```{r, eval=FALSE}
Surv(time = a, time2 = b, event = c, type = d, origin = e)
```

Since $t_i$, $u_i$, and $v_i$ are arbitrary numbers, we cannot pass them
to the function via the `event` or `type` arguments. Hence, $t_i$,
$u_i$, and $v_i$ can only be passed to the function using the `time`,
`time2`, and `origin` arguments.

Further, it is reasonable to presume that $t_i$, $u_i$, and $v_i$ are
passed to the function via the `time`, `time2`, and `origin` arguments
in a bijective manner. That is, the `Surv` function is not implemented
in a way such that, for example, the data contained in the $t_i$ and
$u_i$ values are cleverly encoded together so they can be passed using a
single argument, e.g. `time`. A quick glance at the source code supports
this premise.

The documentation and source code show that the `origin` argument simply
shifts the time scale, by subtracting from `time` and `time2`.
Therefore, this argument cannot be used to specify left truncation or
any of the interval endpoints. In addition, the `if` statement in the
source code reveals that left truncation requires `type='counting'`
which does not allow for interval censoring. On the other hand, interval
censoring requires `type='interval'` which does not allow for left
truncation.

We are left with only two arguments (`time` and `time2`) for passing the
three values. By the pigeonhole principle, it is not possible to pass
three values using only two arguments. Hence, our initial assumption is
incorrect. It is not possible to express the tuple $(t_i,u_i,v_i)$ using
the `Surv` function.

<!-- TODO Argument can prolly be made more solid -->

# C.1.

```{=tex}
\begin{align*}
    & P(T>t \,|\, T>t_0) \\
    =& \frac{P(\{T>t\} \cap \{T>t_0\})}{P(T>t_0)} \\
    =& \frac{P(T > \max(t_0,t))}{P(T>t_0)} \\
    =& \begin{cases}\frac{P(T > t)}{P(T>t_0)} \text{ if }t_0\leq t \\ \frac{P(T > t_0)}{P(T>t_0)} \text{ if }t_0>t\end{cases} \\
    =& \frac{P(T > t)}{P(T>t_0)}𝟙_{t_0\leq t} + \frac{P(T > t_0)}{P(T>t_0)}𝟙_{t_0> t} \\
    =& \frac{P(T > t)}{P(T>t_0)}𝟙_{t_0\leq t} + 𝟙_{t_0> t} \\
    =& \frac{P(T > t)}{P(T>t_0)}𝟙_{t_0\leq t} + 1-𝟙_{t_0\leq  t} \\
    =& \left(\frac{P(T > t)}{P(T>t_0)}-1\right)𝟙_{t_0\leq  t}+1 \\
    =& \left(\frac{S(t)}{S(t_0)}-1\right)𝟙_{t_0\leq  t}+1 \\
\end{align*}
```
If $t_0\leq t$, the expression simplifies to:
$$P(T>t \,|\, T>t_0) = \frac{S(t)}{S(t_0)}$$

# C.2.

We know that $Q(q) = S^{-1}(1-q)$ for any $q$. Equivalently,
$Q(1-q) = S^{-1}(1-(1-q)) = S^{-1}(q)$. With this formula, and with
$t=Q(p\,|\,t_0)$ and assuming $t_0\leq t$, we get:

```{=tex}
\begin{align*}
    & P(T> t \,|\, T>t_0) = 1-p \\
    \Rightarrow& \frac{S(t)}{S(t_0)} = 1-p \\
    \Rightarrow& S(t) = (1-p)S(t_0) \\
    \Rightarrow& S(Q(p\,|\,t_0)) = (1-p)S(t_0) \\
    \Rightarrow& Q(p\,|\,t_0) = S^{-1}((1-p)S(t_0)) \\
    \Rightarrow& Q(p\,|\,t_0) = Q(1-(1-p)S(t_0))  \\
\end{align*}
```
<!-- TODO Not accounting for t0>t -->

# C.3.

We can rewrite the previous formula for $Q(p\,|\,t_0)$ using the CDF:
$$Q(p\,|\,t_0) = Q(1-(1-p)(1-F_T(t_0)))$$ Given
$T\sim \text{LogNormal}(\mu,\sigma^2)$, `plnorm(t0,mu,sigma)` computes
$F_T(t_0)$, and `qlnorm(q,mu,sigma)` computes $Q(q)$.

We can therefore compute $Q(t|t_0)$ like so:

```{r}
p <- 0.4
t0 <- 2
mu <- 1
sigma <- 1.2

cdf_t0 <- plnorm(t0, meanlog = mu, sdlog = sigma)
prob <- 1 - (1 - p) * (1 - cdf_t0)
qlnorm(prob, meanlog = mu, sdlog = sigma)
```

Thus: $$Q(0.4\,|\,2) = Q(1-(1-0.4)(1-F_T(2))) \approx 4.2$$

<!-- TODO Not accounting for t0>t -->

# C.4.

By sampling from the truncated distribution, we can use the empirical
quantile to approximate the true quantile.

```{r}
#' @param n the number of random numbers
#' @param meanlog mean on the log scale
#' @param sdlog sd on the log scale
#' @param t0 left truncation time(s)
#' @return vector of random numbers drawn from a truncated log-normal distribution
rtrunc_lnorm <- function(n, meanlog, sdlog, t0) {
  y <- rlnorm(n, meanlog, sdlog)
  while (any(y < t0)) {
    y[y < t0] <- rlnorm(n, meanlog, sdlog)[y < t0]
  }
  y
}

p <- 0.4
t0 <- 2
mu <- 1
sigma <- 1.2
n <- 1e6

set.seed(7)
sample <- rtrunc_lnorm(n, mu, sigma, t0)
quantile(sample, probs = p)
```

With a sufficiently large sample size ($n=10^6$), wee see that
`quantile` gives us an empirical 40th percentile which is approximately
equal to our value in C3: $$4.178294 \approx 4.171994$$

# D.1.

The partial likelihood in the Cox model with time-varying effects is a
product over the event times:

$$L(\boldsymbol{\beta}) = \prod_{i=1}^n \left(\frac{\exp(\mathbf{x}_i(t_i)^\top\boldsymbol{\beta})}{\sum_{j\in R(t_i)} \exp(\mathbf{x}_j(t_i)^\top\boldsymbol{\beta})}\right)^{\delta_i}$$

where

-   $i$ indexes the individuals,
-   $t_i$ is the observed time for individual $i$,
-   $\delta_i=1$ if individual $i$ experienced the event at $t_i$ and
    $0$ otherwise,
-   $\mathbf{x}_i(t)$ is the time-varying covariate vector for
    individual $i$ at time $t$,
-   $\boldsymbol{\beta}$ is a vector of regression coefficients,
-   $R(t_i)$ is the risk set at time $t_i$.

<!-- TODO Clearly defined notation?? -->

<!-- TODO Justification needed? -->

# D.2.

Let $\boldsymbol{\beta} = (\beta_1,\ldots,\beta_p)$,
$\mathbf{x}_i(t) = (x_{i,1}(t),\ldots,x_{i,p}(t))$ for all $t$ and
$i\in\{1,\ldots,n\}$.

Partial log-likelihood: \begin{align*}
    & \ln L(\boldsymbol{\beta}) \\
    =& \ln \prod_{i=1}^n \left(\frac{\exp(\mathbf{x}_i(t_i)^\top\boldsymbol{\beta})}{\sum_{j\in R(t_i)} \exp(\mathbf{x}_j(t_i)^\top\boldsymbol{\beta})}\right)^{\delta_i} \\
    =& \sum_{i=1}^n \delta_i\ln\frac{\exp(\mathbf{x}_i(t_i)^\top\boldsymbol{\beta})}{\sum_{j\in R(t_i)} \exp(\mathbf{x}_j(t_i)^\top\boldsymbol{\beta})} \\
    =& \sum_{i=1}^n \delta_i\left(\ln\exp(\mathbf{x}_i(t_i)^\top\boldsymbol{\beta}) - \ln \left(\sum_{j\in R(t_i)} \exp(\mathbf{x}_j(t_i)^\top\boldsymbol{\beta})\right)\right) \\
    =& \sum_{i=1}^n \delta_i\left(\mathbf{x}_i(t_i)^\top\boldsymbol{\beta} - \ln \left(\sum_{j\in R(t_i)} \exp(\mathbf{x}_j(t_i)^\top\boldsymbol{\beta})\right)\right) \\
    =& \sum_{i=1}^n \delta_i\left(\sum_{s=1}^p x_{i,s}(t_i)\beta_s - \ln \left(\sum_{j\in R(t_i)} \exp\left(\sum_{s=1}^p x_{j,s}(t_i)\beta_s\right)\right)\right) \\
\end{align*}

Let
$$g(\boldsymbol{\beta}) = \sum_{j\in R(t_i)} \exp\left(\sum_{s=1}^p x_{j,s}(t_i)\beta_s\right)$$
with derivative:

```{=tex}
\begin{align*}
    & \frac{d}{d\beta_k}g(\boldsymbol{\beta}) \\
    =& \frac{d}{d\beta_k}\sum_{j\in R(t_i)} \exp\left(\sum_{s=1}^p x_{j,s}(t_i)\beta_s\right) \\
    =& \sum_{j\in R(t_i)} \left(\frac{d}{d\beta_k}\sum_{s=1}^p x_{j,s}(t_i)\beta_s\right)\exp\left(\sum_{s=1}^p x_{j,s}(t_i)\beta_s\right) \\
    =& \sum_{j\in R(t_i)} x_{j,k}(t_i)\exp\left(\sum_{s=1}^p x_{j,s}(t_i)\beta_s\right) \\
\end{align*}
```
Score: \begin{align*}
    & \frac{d}{d\beta_k}\ln L(\boldsymbol{\beta}) \\
    =& \frac{d}{d\beta_k}\sum_{i=1}^n \delta_i\left(\sum_{s=1}^p x_{i,s}(t_i)\beta_s - \ln g(\boldsymbol{\beta})\right) \\
    =& \sum_{i=1}^n \delta_i\left(\frac{d}{d\beta_k}\sum_{s=1}^p x_{i,s}(t_i)\beta_s - \frac{d}{d\beta_k}\ln g(\boldsymbol{\beta})\right) \\
    =& \sum_{i=1}^n \delta_i\left(x_{i,k}(t_i) - \frac{\frac{d}{d\beta_k}g(\boldsymbol{\beta})}{g(\boldsymbol{\beta})}\right) \\
    =& \sum_{i=1}^n \delta_i\left(x_{i,k}(t_i) - \frac{\sum_{j\in R(t_i)} x_{j,k}(t_i)\exp\left(\sum_{s=1}^p x_{j,s}(t_i)\beta_s\right)}{\sum_{j\in R(t_i)} \exp\left(\sum_{s=1}^p x_{j,s}(t_i)\beta_s\right)}\right) \\
    =& \sum_{i=1}^n \delta_i\left(x_{i,k}(t_i) - \frac{\sum_{j\in R(t_i)} x_{j,k}(t_i)\exp\left(\mathbf{x}_j(t_i)^\top \boldsymbol{\beta}\right)}{\sum_{j\in R(t_i)} \exp\left(\mathbf{x}_j(t_i)^\top \boldsymbol{\beta}\right)}\right) \\
\end{align*}

# D.3.

For the Cox proportional hazards model with time-varying effects, the
hazard of individual $i$ at time $t$ is given by:
$$h(t|\mathbf{x}_i(t)) = h_0(t)\exp(\boldsymbol{\beta}^\top \mathbf{x}_i(t))$$

In this case, we know that $\mathbf{x}_i(t) = (z_i,z_i t)$ where
$z_i\in\{0,1\}$. Let the corresponding coefficients be
$\boldsymbol{\beta} = (\beta_1,\beta_2)$.

```{=tex}
\begin{align*}
    & h(t|\mathbf{x}_i(t)) \\
    =& h_0(t)\exp(\boldsymbol{\beta}^\top \mathbf{x}_i(t)) \\
    =& h_0(t)\exp\left(\begin{bmatrix}\beta_1 & \beta_2\end{bmatrix}\begin{bmatrix}z_i \\ z_i t\end{bmatrix}\right) \\
    =& h_0(t)\exp\left(\beta_1 z_i + \beta_2 z_i t\right) \\
\end{align*}
```
Hazard ratio:

```{=tex}
\begin{align*}
    & \frac{h(t|(1,t))}{h(t|(0,0))} \\
    =& \frac{h_0(t)\exp\left(\beta_1 \cdot 1 + \beta_2 \cdot 1 t\right)}{h_0(t)\exp\left(\beta_1 \cdot 0 + \beta_2 \cdot 0 \cdot t\right)} \\
    =& \exp\left(\beta_1 + \beta_2 t\right) \\
\end{align*}
```
# D.4.

Let
$$\mathbf{x}(t) = (\text{stage}_{\text{Unk}},\text{stage}_{\text{Reg}},\text{stage}_{\text{Dis}},\text{stage}_{\text{Dis}} \cdot t)$$
where each $\text{stage}_{i} \in \{0,1\}$.

Regression model:

$$\ln h(t|\mathbf{x}(t)) = \ln h_0(t) + \beta_{\text{Unk}} \text{stage}_{\text{Unk}} + \beta_{\text{Reg}}\text{stage}_{\text{Reg}} + \beta_{tt}\text{stage}_{\text{Dis}} t$$

$h_0$ denotes the baseline hazards function. The coefficients
$\beta_{\text{Unk}}$, $\beta_{\text{Reg}}$, $\beta_{\text{Dis}}$ are
log-hazard ratios for the "Unknown", "Regional" and "Distant" stages,
relative to the reference
stage "Localised", at time $t=0$.

$\beta_{tt}$ is the change in the log-hazard ratio for the "Distant" stage
(relative to "Localised") per year increase in survival time.

```{r, echo=FALSE, message=FALSE}
library(survival)
library(biostat3)
```

```{r}
model <- coxph(Surv(surv_mm, status == "Dead: cancer") ~ stage + tt(stage),
  data = transform(biostat3::colon, stage = relevel(stage, "Localised")),
  tt = function(x, t, ...) (x == "Distant") * t / 12
)
summary(model)
```

Interpretation:

-   $\beta_{\text{Unk}} \approx 0.94$ is the estimated log-hazard ratio
    for the "Unknown" stage. The estimated hazard ratio is
    $\exp(\beta_{\text{Unk}}) \approx 2.56$. Meaning, the hazard for
    colon cancer patients in the "Unknown" stage is 2.56 times that of
    patients in the "Localised" stage.
-   $\beta_{\text{Reg}} \approx 0.80$ is the estimated log-hazard ratio
    for the "Regional" stage. The estimated hazard ratio is
    $\exp(\beta_{\text{Reg}}) \approx 2.23$. Meaning, the estimated
    hazard for colon cancer patients in the "Regional" stage is 2.23
    times that of patients in the "Localised" stage.
-   $\beta_{\text{Dis}} \approx 2.22$ is the estimated log-hazard ratio
    for the "Distant" stage. The estimated hazard ratio is
    $\exp(\beta_{\text{Dis}}) \approx 9.20$. Meaning, the estimated
    hazard for colon cancer patients in the "Distant" stage is 9.20
    times that of patients in the "Localised" stage at time $t=0$.
-   $\beta_{tt} \approx -0.12$ is the estimated per-year change in the
    log-hazard ratio for "Distant" vs. "Localised". The corresponding
    estimaed change in hazard ratio is $\exp(\beta_{tt}) \approx 0.88$.
    Meaning, each additional year of follow-up multiplies the hazard
    ratio for "Distant" vs. "Localised" by about 0.88. In other words,
    the hazard ratio for "Distant" vs. "Localised" decreases by about 12
    % each year after $t=0$.

The p-value of each coefficient is well below $0.05$, indicating very
strong statistical significance.

# E.1.

We use `survfit` to fit Kaplan-Meier survival curves stratified by the
binary variable `hormon`, then plot the curves using `ggsurvplot`:

```{r, message=FALSE}
library(survival)
library(survminer)
library(rstpm2)
```

```{r}
data(brcancer)
surv_object <- Surv(time = brcancer$rectime, event = brcancer$censrec)
km_fit <- survfit(surv_object ~ hormon, conf.type="log-log", data = brcancer)
ggsurvplot(
  km_fit,
  data = brcancer,
  pval = TRUE,
  risk.table = FALSE,
  conf.int = TRUE,
  legend.labs = c("No hormonal therapy", "Hormonal therapy"),
  legend.title = "Randomization arm",
  xlab = "Time (days)",
  ylab = "Survival probability",
  title = "Kaplan-Meier survival curves by randomization arm"
)
```

::: {style="text-align: center;"}
*Figure 1: Kaplan-Meier curves by randomization arm.*
:::

Figure 1 shows the following:

-   The probability of survival decreases over time for both arms,
    indicating that the risk of breast cancer recurrence increases with
    longer follow-up.
-   The curve for the experimental group (`hormon=1`) is consistently
    above the curve for the control group (`hormon=0`), indicating that
    patients receiving hormonal therapy tend to have higher survival
    probabilities throughout the follow-up period compared to those who
    did not receive hormonal therapy.
-   The curves do not intersect and appear to maintain a consistent
    separation over time, possibly suggesting proportional hazards.
-   The difference in survival probabilities appears to be the most
    pronounced in the period from \~1300 to \~2000 days, where the 95 %
    confidence intervals overlap minimally.
-   The confidence intervals are wider at later timepoints, reflecting
    increased uncertainty due to fewer patients remaining in the study.
-   The curve for the control group includes a steep vertical jump
    toward the end, indicating that the number of patients at risk in
    this group is in the single-digits at this point in time.
-   There seem to be a sudden drop in survival for the experimental
    group at around 2000 days. One explanation is that the effect of
    hormonal therapy on survival diminishes after around 2000 days. But
    this could also be coincidental -- as the study approaches later
    time points, random variation plays a bigger role because fewer
    patients remain in the risk set.
-   The p-value of the log-rank test is $p=0.0034 < 0.05$, suggesting
    strong evidence against the null hypothesis that hormonal therapy
    has no effect on survival. There is strong evidence that hormonal
    therapy is associated with an improvement in recurrence-free
    survival.

<!-- TODO Switch from days -> years? That's what he does in his examples -->

# E.2.

We are interested in learning whether hormonal therapy has an effect on
the survival of breast cancer patients, and the magnitude of the effect.
A Cox proportional hazards model is appropriate if we assume
proportional hazards for the two groups over time. Our estimand of
choice is therefore the hazard ratio of hormonal therapy treatment vs.
no treatment.

Since this is a randomized controlled trial, we can presume that
the assignment of hormonal therapy is independent of
measured and unmeasured confounding variables
thanks to randomization. This simplifies our model, as there is no
need to adjust for additional covariates besides the exposure variable
in order to obtain an unbiased estimate of the treatment effect.
We assume here that our estimand of interest is the population-level effect
of hormonal treatment, as opposed to a conditional effect
within specific strata such as age groups.

Let $\text{hormon}=1$ if the subject received hormonal therapy and
$\text{hormon}=0$ otherwise. Let $\beta_{\text{hormon}}$ be the
corresponding regression coefficient.

Let $h(t|\text{hormon})$ denote the hazard at time $t$. $h_0(t)$ is the
baseline hazard (when $\text{hormon}=0$).

Regression model:

$$h(t|\text{hormon}) = h_0(t)\exp(\beta_{\text{hormon}} \text{hormon})$$

In this model, $\beta_{\text{hormon}}$ is a log-hazard ratio, so our
estimand of interest is $\exp(\beta_{\text{hormon}})$.

Fitting the regression model:

```{r}
cox_model <- coxph(surv_object ~ hormon, data = brcancer)
summary(cox_model)
```

Hazard ratio for `hormon=1` vs. `hormon=0`:

```{r}
exp(coef(cox_model))
```

95 % confidence interval:

```{r}
exp(confint(cox_model))
```

The estimate of the hazard ratio is
$\exp(\hat{\beta}_{\text{hormon}}) \approx 0.69$, with a 95 % CI of
$[0.54, 0.89]$. This indicates that hormonal therapy lowers the hazard
by around 31 % compared to lack of treatment. The confidence interval
does not include 1, and the p-value is significant at $p=0.0036 < 0.05$,
indicating that the effect is statistically significant.

<!-- TODO Should I include other covariates? -->

<!-- https://cran.r-project.org/web/packages/rstpm2/vignettes/SimpleGuide.pdf -->

<!-- https://cran.r-project.org/web/packages/rstpm2/vignettes/Introduction.pdf -->

# E.3.

One way to test for proportional hazards is to fit an extended Cox model
with an explicit time-by-covariate interaction term. We would then
perform a likelihood ratio test to attempt to reject the null hypothesis
that the model without time-varying effects is adequate. However, this
requires selecting a specific form of the time-varying hazard ratio, and
it may not be obvious how to specify it.

A more model-agnostic approach is to perform a Schoenfeld residuals
test. This test checks for time dependence in covariate effects
non-parametrically by examining the correlation between scaled
Schoenfeld residuals and time. The Cox model assumes that the effect of
hormonal treatment on the hazard is constant over time. The Schoenfeld
residuals should then have no correlation with time if the proportional
hazards assumption holds.

When performing the test, we should choose scaled residuals over
unscaled residuals. Grambsch and Thereau [@grambsch1994] showed that
scaled residuals have better properties such as approximately constant
variance over time. This means standard statistical methods can be
applied, including a chi-square test. We can also consider whether we
want to scale time in the residual regression. Using an identity
transform (`transform = "identity"`) regresses scaled Schoenfeld
residuals on raw time, which is appropriate when we are interested in
assessing or interpreting time-varying effects on the original time
scale. Alternatively, the Kaplan-Meier transform (`transform = "km"`)
uses the Kaplan-Meier estimate of survival as the time scale, which can
improve power and robustness in the presence of heavy or nonuniform
censoring. This transformation reflects the amount of information
available at each time point and is particularly useful when the hazard
function or censoring distribution is highly variable, which may be the
case here.

In our scenario, we want to use the test to check whether the effect of
the covariate (`hormon`) changes over time:

```{r}
ph_test <- cox.zph(cox_model, transform = "km")
ph_test
```

Since $p=0.63$, we fail to reject the null hypothesis that the
proportional hazards assumption holds for the hormonal therapy treatment
variable. Meaning, there is no significant evidence that the effect of
hormonal treatment changes over time. Hence, the proportional hazards
assumption appears to be satisfied, making the Cox model suitable. We
can therefore trust the hazard ratio estimate we found earlier to be a
valid measure of the effect of hormonal therapy on survival.

# E.4.

We can plot the Schoenfeld residuals against time to visually inspect
the proportional hazards assumption. If the assumption is met, the
residuals for the `hormon` covariate should show no systematic trend
over time, i.e. no slope. If there is a time-dependent effect, we expect
to see a non-horizontal pattern in the residuals. The advantage of this
method is that it is specifically designed for Cox models, it
complements the formal test (using `cox.zph(...)`), and it can be used
for each covariate separately, helping isolate potential violations.

```{r}
plot(ph_test)
```

::: {style="text-align: center;"}
*Figure 2: Residuals for* $\beta_{\text{hormon}}$.
:::

The Loess-smoothed curve in Figure 2 resembles a horizontal line, and
the confidence band does not substantially deviate from it. Thus, there
is no evidence that the hazard ratio changes over time, making the
proportional hazards assumption valid.

<!-- TODO balanced motivation? -->

# F.1.

We first need to determine the estimand of interest. In this scenario,
we want to investigate the effect of an exposure on the survival of
colorectal cancer patients. Our exposure variable $X$ is binary, with
$X=1$ indicating aspirin treatment and $X=0$ indicating no treatment.
Our response variable $T$ is time to cause-specific death. We are
therefore interested in estimating a measure of association between
these two variables. There are several candidates: hazard ratio, hazard
difference, survival probability difference, restricted mean survival
time (RMST) difference. The standard choice of measure for this type of
study would be the hazard ratio. Hazard difference carries the
disadvantage of being unstable over time and thus difficult to
interpret, as hazards generally change over time. Survival probability
does not account for competing risks, such as death due to other causes.
RMST difference could be a valuable alternative if hazards are
non-proportional, but requires carefully selecting a suitable time
horizon $\tau$. Since the event of interest is cause-specific death, we
are specifically interested in the cause-specific hazard ratio. This
means that we should treat death due to other causes as a competing risk
and censor such events.

The hazard ratio at time $t$ is given by:

$$\text{HR}(t|X) = \frac{h(t|X=1)}{h(t|X=0)}$$

Next, we need to find a suitable estimator. Since this is a randomized
controlled trial with potential right-censoring, we will start off with
the standard approach of assuming proportional hazards and the Cox
proportional hazards model. This means the hazard rate can be expressed
with the formula:

$$h(t|X) = h_0(t)\exp(\beta X)$$

where $h_0(t)$ is the baseline hazard at $t$ and $\beta$ is the
regression parameter corresponding to $X$.
Our estimator is then the random variable $\exp(\hat{\beta})$,
where \(\hat{\beta}\) denotes an estimator of the coefficient \(\beta\) obtained by fitting a Cox model.

Since this is a randomized controlled trial, the values of explanatory
variables should be balanced between treatment groups. Hence, on
average, both measured and unmeasured sources of confounding, denoted
$U$, are expected to be equally distributed across treatment groups.
However, this is only true at time $t=0$. The hazard $h(t|X)$ at time
$t$ is conditional on surviving to time $t$, so the survival time $S(t)$
acts as a collider which we implicitly condition on. Hence, at time
$t+\Delta$, where $\Delta>0$, there is an open path:
$$X \to S(t) \leftarrow U \to S(t+\Delta) $$ which introduces a spurious
association between $X$ and $U$. To assess the magnitude of this
confounding bias, we could use the Cox-specific estimator based on
the G-computation formula proposed by Martinussen et al. [@martinussen2013].

We also need to consider that the number of subjects with both
colorectal cancer and the genetic signature may be small. It is
therefore possible that we will have to work with a small sample size.
Hence, the values of some prognostic covariates may still be unevenly
distributed by chance. To determine if this is the case, we would check
baseline characteristics (age, sex, stage at diagnosis, etc.) for
imbalances between the experimental arm and the control arm. We would do
this by producing a baseline table (Table 1) comparing these
characteristics by randomization arm. We would then determine if strong
imbalances are present using the standardized mean difference (SMD)
metric. A SMD of, say, 0.2 or greater would be interpreted as a strong
imbalance. If strong imbalances exist, we would first check for errors
in our randomization or analysis and redo them if necessary. If no
errors were found, we would consider adjusting for these covariates.

Next, we would investigate whether the proportional hazards assumption
holds.
We would start by fitting a Cox proportional hazards model and
making a table of the regression coefficients,
hazard ratios, confidence intervals and p-values.
We would then plot the Kaplan-Meier curve to visualize survival
over time for
the two groups. We would then inspect the survival
curves, perform a Schoenfeld residual test, and plot the residuals
against time. If the survival curves do not cross, the residual test is
non-significant, and the residual plot shows a horizontal pattern, we
would assume that the proportional hazards assumption is met.

If proportional hazards are violated, we would need to consider
alternative models.
We would switch to a stratified Cox regression model to check
if it fits better.
If not, we would try a time-dependent Cox model.
If that also fails, an RMST analysis could be a viable alternative,
as it does not depend on the proportional hazards assumption.

<!-- lecture_miscel slide 13 -->

<!-- We adjust for S(t), which acts as a collider? -->

<!-- To Cox-stratify or adjust? -->

<!-- Small sample size -- adjust for prognostic variables for the sake of improving precision/power? -->

<!-- Intent-to-treat vs per-protocol -->

<!-- Alt: stratified Cox regression -->

<!-- Alt: time-dependent Cox regression -->

<!-- Alt: RMST analysis -->

<!-- Alt: AFT model? -->

# F.2.

We proposed the hazard ratio from a Cox proportional hazards model
as the estimator of treatment effect.
Hazard ratios are non-collapsible [@martinussen2013], meaning
that the marginal estimate differs from the conditional estimate
even in the absence of confounding.
This is potentially an issue because a policymaker might care about
the population-level effect of the treatment (unadjusted hazard ratio),
whereas
a clinician might be more interested in the effect conditional
on patient characteristics (adjusted hazard ratio).
Hence, if we are measuring both the adjusted and unadjusted effect,
we run the risk of misinterpreting the size of the effect.

Non-collapsibility is negligible if the event of interest is rare
or if the frailty is small.
Considering that the study goes on for 5 years, and the event of interest
is cause-specific death, the event can be considered moderately rare.
The size of the frailty is harder to assess. Since the study
focuses on a specific genetic signature, this might reduce some heterogeneity.
But unless the sample is very homogeneous, some frailty likely remains.

Hence, as the events are rare, we can expect the non-collapsibility
to be less of an issue.
If we wanted to address non-collapsibility, we could
switch to another model which involves a collapsible measure of association.
Examples include:
the Aalen's additive hazards model, an accelerated failure time model,
or the RMST difference.
There would be drawbacks to each of these alternatives, such as
the fact that the Aalen model is based on the hazard difference,
and thus arguably harder to interpret clinically.

<!-- TODO lecture_miscel.pdf slide 33 -->

<!-- Estimand: hazard ratio, hazard diff, risk diff... -->

<!-- How to distinguish death vs. cause-specific death? -->

# References
